#!/bin/bash -l
# FILENAME:  Solve_QCQP_Our_Algorithm_Parallel_Core_128_part_2.sub
#SBATCH --ntasks=129
#SBATCH --cpus-per-task=1
#SBATCH --nodes=7
#SBATCH --ntasks-per-node=19
#SBATCH --exclusive
#SBATCH --time=04:00:00
#SBATCH -A standby

cd $SLURM_SUBMIT_DIR

module --force purge
module load intel
module load impi
module load utilities monitor
module list

# start memory resource monitor
monitor cpu memory --csv | head -1 >${SLURM_JOB_NAME}.${SLURM_JOB_ID}.memory.csv
mpiexec -machinefile <(srun hostname | sort -u) \
	monitor cpu memory --csv --no-header \
	>>${SLURM_JOB_NAME}.${SLURM_JOB_ID}.memory.csv &
MEM_PID=$!

# start cpu resource monitor
# monitor cpu percent --all-cores --csv | head -1 >${SLURM_JOB_NAME}.${SLURM_JOB_ID}.cores.csv
# mpiexec -machinefile <(srun hostname | sort -u) \
# 	 monitor cpu percent --all-cores --csv --no-header \
# 	 >>${SLURM_JOB_NAME}.${SLURM_JOB_ID}.cores.csv &
# CPU_PID=$!

# Method 1:
# export I_MPI_PROCESS_MANAGER=mpd
# mpirun -n 129 ./Solve_QCQP_Our_Algorithm_Parallel_Core_128_part_2

# Method 2:
mpirun -bootstrap slurm -n 129 ./Solve_QCQP_Our_Algorithm_Parallel_Core_128_part_2

# Method 3:
# mpiexec.hydra -bootstrap slurm -n 129 ./Solve_QCQP_Our_Algorithm_Parallel_Core_128_part_2

# Method 4:
# export I_MPI_PMI_LIBRARY=/usr/lib64/libpmi.so
# srun -n 129 ./Solve_QCQP_Our_Algorithm_Parallel_Core_128_part_2

# Method 5:
#srun --mpi=pmi2 -n 129 ./Solve_QCQP_Our_Algorithm_Parallel_Core_128_part_2

# wait 10  minutes
sleep 600

# stop resource monitors
kill -s INT $MEM_PID #$CPU_PID

